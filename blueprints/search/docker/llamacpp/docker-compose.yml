services:
  llamacpp-quick:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "8082:8080"
    volumes:
      - ${HOME}/data/models:/models:ro
    command: >
      --host 0.0.0.0
      --port 8080
      --model /models/gemma-3-270m-it.gguf
      --ctx-size 4096
      --embedding
      --pooling mean
      --cont-batching
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  llamacpp-deep:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "8083:8080"
    volumes:
      - ${HOME}/data/models:/models:ro
    command: >
      --host 0.0.0.0
      --port 8080
      --model /models/gemma-3-1b-it.gguf
      --ctx-size 8192
      --embedding
      --pooling mean
      --cont-batching
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  llamacpp-research:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "8084:8080"
    volumes:
      - ${HOME}/data/models:/models:ro
    command: >
      --host 0.0.0.0
      --port 8080
      --model /models/gemma-3-4b-it.gguf
      --ctx-size 16384
      --embedding
      --pooling mean
      --cont-batching
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  llamacpp-gpt-oss:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "8085:8080"
    volumes:
      - ${HOME}/data/models:/models:ro
    command: >
      --host 0.0.0.0
      --port 8080
      --model /models/gpt-oss-20b.gguf
      --ctx-size 32768
      --cont-batching
      --n-gpu-layers 99
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 16G
